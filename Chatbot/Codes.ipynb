{"cells":[{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T11:47:18.338838Z","iopub.status.busy":"2023-10-05T11:47:18.338450Z","iopub.status.idle":"2023-10-05T11:47:18.345449Z","shell.execute_reply":"2023-10-05T11:47:18.343971Z","shell.execute_reply.started":"2023-10-05T11:47:18.338811Z"},"trusted":true},"outputs":[],"source":["# This file was written in Kaggle notebook.\n","# This is the main file of the chatbot.\n","# This file contains the code for training the model and predicting the output.\n","# This file also contains the code for the chatbot to interact with the user.\n","\n","# Importing the libraries\n","# if the libraries are not installed in Kaggle notebook, then install them using \"!pip install <library name>\"\n","import numpy as np # np for creating the arrays\n","import nltk # Natural Language Toolkit for tokenizing the words\n","from nltk.stem.lancaster import LancasterStemmer # Lancaster Stemmer for stemming the words\n","import tensorflow as tf # Tensorflow for creating the model\n","import random # Random for randomizing the responses\n","import json # JSON for reading the intents file\n","import tflearn # TFLearn for training the model\n","import pickle # Pickle for saving the model"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T11:48:15.618062Z","iopub.status.busy":"2023-10-05T11:48:15.617629Z","iopub.status.idle":"2023-10-05T11:48:15.631480Z","shell.execute_reply":"2023-10-05T11:48:15.630303Z","shell.execute_reply.started":"2023-10-05T11:48:15.618033Z"},"trusted":true},"outputs":[],"source":["# Creating an object of Lancaster Stemmer\n","stemmer = LancasterStemmer()\n","# putting the path of the intents file in the variable\n","kaggle_directory = \"/kaggle/input/\"\n","# Reading the intents file\n","with open(kaggle_directory+\"article/intents.json\") as file:\n","    data = json.load(file)"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T11:48:18.740837Z","iopub.status.busy":"2023-10-05T11:48:18.739688Z","iopub.status.idle":"2023-10-05T11:48:18.842526Z","shell.execute_reply":"2023-10-05T11:48:18.840967Z","shell.execute_reply.started":"2023-10-05T11:48:18.740788Z"},"trusted":true},"outputs":[],"source":["words = []\n","labels = []\n","docs_x = []\n","docs_y = []\n","\n","# Preprocessing the data\n","# data[\"intents\"] is the value of the key \"intents\" in the dictionary data. This is a list of dictionaries itself.\n","# Each element in this list, is a dictionary which contains the keys \"tag\", \"patterns\" and \"responses\".\n","for intent in data[\"intents\"]:\n","    # for every pattern in the list of patterns we will tokenize the words and add them to the list of words\n","    for pattern in intent[\"patterns\"]:\n","        tokens = nltk.word_tokenize(pattern)\n","        words.extend(tokens)\n","        # we will add the tokenized words to the list of docs_x and the corresponding tag to the list of docs_y\n","        docs_x.append(tokens)\n","        docs_y.append(intent[\"tag\"])\n","    # labels will contain all the tags\n","    if intent[\"tag\"] not in labels:\n","        labels.append(intent[\"tag\"])\n","# stemming the words and removing the duplicates.\n","# we will sort the list of words and labels and remove the the question mark from the list of words\n","words = sorted (list (set ([stemmer.stem(w.lower()) for w in words if w != \"?\"])))\n","labels = sorted (labels)\n","\n","# print debugging statements\n","# print(words)\n","# print(labels)\n","# print(docs_x)\n","# print(docs_y)\n","\n","# defining the training and output lists\n","training = []\n","output = []\n","out_empty = [0 for _ in range(len(labels))]\n","\n","# creating the bag of words\n","for x, doc in enumerate(docs_x):\n","    bag = []\n","    tokens = [stemmer.stem(w) for w in doc]\n","    # for every word in the list of words, we will append the number of times it occurs in the list of tokens to the bag.\n","    for w in words:\n","        bag.append(tokens.count(w))\n","\n","    # output_row will be a list of 0s with the index of the tag in the list of labels as 1\n","    output_row = out_empty[:]\n","    output_row[labels.index(docs_y[x])] = 1\n","\n","    # appending the bag and output_row to the training and output lists respectively\n","    training.append(bag)\n","    output.append(output_row)\n","\n","training = np.array(training)\n","output = np.array(output)\n","\n","# saving the preprocessed data in a pickle file\n","with open(\"data.pickle\", \"wb\") as f:\n","    pickle.dump((words, labels, training, output), f)\n","    \n","# we do not need to preprocess the data if it is already preprocessed\n","# so we will save the preprocessed data in a pickle file and load it from there\n","# if the pickle file is not present, then we will preprocess the data and save it in the pickle file\n","# note that if the intents file is changed, then the pickle file will not be updated. This should be done manually by deleting the pickle file.\n","# try:\n","#     with open(\"data.pickle\", \"rb\") as f:\n","#         words, labels, training, output = pickle.load(f)\n","# except: put the preprocessing in the except."]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T11:48:33.275169Z","iopub.status.busy":"2023-10-05T11:48:33.273752Z","iopub.status.idle":"2023-10-05T11:48:33.701146Z","shell.execute_reply":"2023-10-05T11:48:33.700287Z","shell.execute_reply.started":"2023-10-05T11:48:33.275117Z"},"trusted":true},"outputs":[],"source":["# resetting the default graph\n","tf.compat.v1.reset_default_graph()\n","# defining the neural network\n","# we create a fully connected neural network with 3 hidden layers\n","# the input layer will have the same number of neurons as the number of words in the list of words\n","# the output layer will have the same number of neurons as the number of tags in the list of labels\n","# the activation function of the output layer will be softmax\n","# the hidden layers will have 8 neurons each.\n","net = tflearn.input_data(shape=[None, len(training[0])])\n","net = tflearn.fully_connected(net, 8)\n","net = tflearn.fully_connected(net, 8)\n","net = tflearn.fully_connected(net, 8)\n","net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n","net = tflearn.regression(net)\n","model = tflearn.DNN(net)"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T11:48:46.343575Z","iopub.status.busy":"2023-10-05T11:48:46.342119Z","iopub.status.idle":"2023-10-05T11:50:44.510680Z","shell.execute_reply":"2023-10-05T11:50:44.509189Z","shell.execute_reply.started":"2023-10-05T11:48:46.343520Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Step: 19999  | total loss: \u001b[1m\u001b[32m0.00034\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 1000 | loss: 0.00034 - acc: 1.0000 -- iter: 152/158\n","Training Step: 20000  | total loss: \u001b[1m\u001b[32m0.00035\u001b[0m\u001b[0m | time: 0.078s\n","| Adam | epoch: 1000 | loss: 0.00035 - acc: 1.0000 -- iter: 158/158\n","--\n"]}],"source":["model.fit(training, output, n_epoch=1000, batch_size = 8, show_metric= True)\n","model.save(\"model_first\")\n","# if the model is already trained, then load it from the file\n","# model.load(\"model_first\")"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T11:54:16.015918Z","iopub.status.busy":"2023-10-05T11:54:16.015466Z","iopub.status.idle":"2023-10-05T11:54:16.023757Z","shell.execute_reply":"2023-10-05T11:54:16.022481Z","shell.execute_reply.started":"2023-10-05T11:54:16.015887Z"},"trusted":true},"outputs":[],"source":["# function for tokenizing the words\n","# the words is the list which was created while preprocessing the data and contains all the words in the list of words\n","def tokenizing(sentence, words):\n","    bag = [0 for _ in range(len(words))]\n","    # tokenizing the words in the sentence and stemming them\n","    w = nltk.word_tokenize(sentence)\n","    w = [stemmer.stem(w.lower()) for w in w if w != \"?\"]\n","    # for every word in the words of the sentence, we will set the value of the corresponding index in the bag to 1\n","    for se in w:\n","        for i, w2 in enumerate(words):\n","            if w2 == se:\n","                bag[i]= 1\n","    return np.array(bag)"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T12:46:22.549765Z","iopub.status.busy":"2023-10-05T12:46:22.549407Z","iopub.status.idle":"2023-10-05T12:50:34.068179Z","shell.execute_reply":"2023-10-05T12:50:34.067043Z","shell.execute_reply.started":"2023-10-05T12:46:22.549740Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Lets start chatting...(type quit to stop)\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  what is the title of the paper?\n"]},{"name":"stdout","output_type":"stream","text":["An Automated Approach to Estimating Code Coverage Measures via Execution Logs\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  what is the paper about?\n"]},{"name":"stdout","output_type":"stream","text":["Automating Code Coverage using Execution Logs\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  what is the overall message of the paper?\n"]},{"name":"stdout","output_type":"stream","text":["It introduces LogCoCo, an automated code coverage estimation approach.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  give me a brief abstract of the paper\n"]},{"name":"stdout","output_type":"stream","text":["HBase was selected for its widespread usage, serving millions of users in companies like Facebook and Twitter.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  give me one strength of LogCoCO\n"]},{"name":"stdout","output_type":"stream","text":["The excessive instrumentation guarantees accurate measurements of code coverage. However, problems like deployment challenges and performance overhead are imposed. LogCoCo on the other hand, is easy to setup and imposes little performance overhead by analyzing the execution logs.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  give me one weakness of LogCoCo\n"]},{"name":"stdout","output_type":"stream","text":["The excessive instrumentation guarantees accurate measurements of code coverage. However, problems like deployment challenges and performance overhead are imposed. LogCoCo on the other hand, is easy to setup and imposes little performance overhead by analyzing the execution logs.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  What are the challenges of LogCoCO?\n"]},{"name":"stdout","output_type":"stream","text":["Generalizability: LogCoCo primarily focuses on server-side systems with extensive logging. To enhance its applicability, research studies can explore ways to adapt LogCoCo to mobile applications and client/desktop-based systems with limited or no logging. This expansion would make LogCoCo a more versatile tool. One way to do this expansion can be through focusing on developing automated tools or techniques that strategically insert logging statements into source code where there are limited logging available. This technique can also be used to reduce the amount of May labels.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  can LogCoCo be used in mobile applications?\n"]},{"name":"stdout","output_type":"stream","text":["LogCoCo's approach of inferring system execution contexts is significant because it allows indirect detection of covered methods.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  What are the issues associated with code coverage tools like JaCoCo in practice?\n"]},{"name":"stdout","output_type":"stream","text":["JaCoCo did not report code coverage measures for some modules, particularly those not directly invoked by the YCSB benchmark suite.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  how many experiments were conducted?\n"]},{"name":"stdout","output_type":"stream","text":["Measuring code coverage in a DevOps-like environment is challenging due to rapid deployment processes.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  what is the Hbase Experiment?\n"]},{"name":"stdout","output_type":"stream","text":["The experiment used the YCSB benchmark suite, originally developed by Yahoo!.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  what are the engineering challenges of using JaCoCO?\n"]},{"name":"stdout","output_type":"stream","text":["The paper discusses software testing and code coverage measures.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  what are the engineering challenges?\n"]},{"name":"stdout","output_type":"stream","text":["I don't understand your question\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  in the paper a breadth for search algorithm was used explain\n"]},{"name":"stdout","output_type":"stream","text":["The HBase experiment used a three-machine-cluster with specific hardware specs.\n"]},{"name":"stdout","output_type":"stream","text":["ask your question:  quit\n"]}],"source":["def chat_with_bot():\n","    print(\"Lets start chatting...(type quit to stop)\")\n","    # main loop of the chatbot\n","    while (True):\n","        inp = input(\"ask your question: \")\n","        # if the user types quit, then break out of the loop\n","        if inp.lower() == \"quit\":\n","            break\n","        # tokenizing the input and predicting the output\n","        # this will actually be a list of probabilities\n","        result = model.predict([tokenizing(inp, words)])[0]\n","        # we pick the index of the maximum probability\n","        result_index = np.argmax(result)\n","        tag = labels[result_index]\n","        # if the probability is greater than 0.7, then we will print the corresponding response\n","        # however, with lower probabilities, the model is not very accurate and so we decided to print \"I don't understand your question\"\n","        if max(result) > 0.6:\n","            for tg in data[\"intents\"]:\n","                if tg[\"tag\"] == tag:\n","                    responses = tg[\"responses\"]\n","                    break\n","            if tg[\"tag\"] == tag:\n","                print(random.choice(responses))\n","        else:\n","            print(\"I don't understand your question\")\n","        \n","chat_with_bot()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
